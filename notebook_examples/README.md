
ADS Expertise Notebooks
=======================

The [Accelerated Data Science (ADS) SDK](https://accelerated-data-science.readthedocs.io/en/latest/) is maintained by the Oracle Cloud Infrastructure Data Science service team. It speeds up common data science activities by providing tools that automate and/or simplify common data science tasks, along with providing a data scientist friendly pythonic interface to Oracle Cloud Infrastructure (OCI) services, most notably OCI Data Science, Data Flow, Object Storage, and the Autonomous Database. ADS gives you an interface to manage the lifecycle of machine learning models, from data acquisition to model evaluation, interpretation, and model deployment.

The ADS SDK can be downloaded from [PyPi](https://pypi.org/project/oracle-ads/), contributions welcome on [GitHub](https://github.com/oracle/accelerated-data-science)

[![PyPI](https://img.shields.io/pypi/v/oracle-ads.svg?style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/oracle-ads/) [![Python](https://img.shields.io/pypi/pyversions/oracle-ads.svg?style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/oracle-ads/)

    


## Topics
<img src="https://img.shields.io/badge/deploy model-8-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/register model-7-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/train model-7-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/automlx-5-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/data flow-4-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/pyspark-4-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/bds-3-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/oracle open data-3-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/scikit learn-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/big data service-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/nlp-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/autonomous database-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/language services-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/string manipulation-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/regex-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/regular expression-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/natural language processing-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/NLP-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/part of speech tagging-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/named entity recognition-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/sentiment analysis-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/custom plugins-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/data catalog metastore-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/xgboost-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/text classification-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/classification-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/regression-2-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/intel-1-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/intel extension-1-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/scikit learn-1-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> <img src="https://img.shields.io/badge/feature%20store-10-brightgreen?style=for-the-badge&logo=pypi&logoColor=white"> 

## Contents
 - [Audi Autonomous Driving Dataset Repository](#audi-autonomous_driving-oracle_open_data.ipynb)
 - [Bank Graph Example Notebook](#graph_insight-autonomous_database.ipynb)
 - [Building a Forecaster using AutoMLx](#automlx-forecasting.ipynb)
 - [Building and Explaining a Classifier using AutoMLx](#automlx-classifier.ipynb)
 - [Building and Explaining a Regressor using AutoMLx](#automlx-regression.ipynb)
 - [Building and Explaining a Text Classifier using AutoMLx](#automlx-text_classification.ipynb)
 - [Building and Explaining an Anomaly Detector using AutoMLx - Experimental](#automlx-anomaly_detection.ipynb)
 - [Caltech Pedestrian Detection Benchmark Repository](#caltech-pedestrian_detection-oracle_open_data.ipynb)
 - [Connect to Oracle Big Data Service](#big_data_service-(BDS)-kerberos.ipynb)
 - [Fairness with AutoMLx](#automlx-fairness.ipynb)
 - [Graph Analytics and Graph Machine Learning with PyPGX](#pypgx-graph_analytics-machine_learning.ipynb)
 - [How to Read Data with fsspec from Oracle Big Data Service (BDS)](#read-write-big_data_service-(BDS).ipynb)
 - [Intel Extension for Scikit-Learn](#accelerate-scikit_learn-with-intel_extension.ipynb)
 - [Introduction to ADSTuner](#hyperparameter_tuning.ipynb)
 - [Introduction to Model Version Set](#model_version_set.ipynb)
 - [Introduction to SQL Magic](#sql_magic-commands-with-autonomous_database.ipynb)
 - [Introduction to Streaming](#streaming-service-introduction.ipynb)
 - [Introduction to the Oracle Cloud Infrastructure Data Flow Studio](#pyspark-data_flow_studio-introduction.ipynb)
 - [Loading Data With Pandas & Dask](#load_data-object_storage-hive-autonomous-database.ipynb)
 - [Model Evaluation with ADSEvaluator](#model_evaluation-with-ADSEvaluator.ipynb)
 - [Natural Language Processing](#natural_language_processing.ipynb)
 - [ONNX Integration with the Accelerated Data Science (ADS) SDK](#onnx-integration-ads.ipynb)
 - [PySpark](#pyspark-data_flow-application.ipynb)
 - [Spark NLP within Oracle Cloud Infrastructure Data Flow Studio](#pyspark-data_flow_studio-spark_nlp.ipynb)
 - [Text Classification and Model Explanations using LIME](#text_classification-model_explanation-lime.ipynb)
 - [Text Classification with Data Labeling Service Integration](#data_labeling-text_classification.ipynb)
 - [Text Extraction Using the Accelerated Data Science (ADS) SDK](#document-text_extraction.ipynb)
 - [Train, Register, and Deploy a Generic Model](#train-register-deploy-other-frameworks.ipynb)
 - [Train, Register, and Deploy a LightGBM Model](#train-register-deploy-lightgbm.ipynb)
 - [Train, Register, and Deploy a PyTorch Model](#train-register-deploy-pytorch.ipynb)
 - [Train, Register, and Deploy a TensorFlow Model](#train-register-deploy-tensorflow.ipynb)
 - [Train, Register, and Deploy an XGBoost Model](#train-register-deploy-xgboost.ipynb)
 - [Train, register, and deploy HuggingFace Pipeline](#train-register-deploy-huggingface-pipeline.ipynb)
 - [Train, register, and deploy Sklearn Model](#train-register-deploy-sklearn.ipynb)
 - [Using Data Catalog Metastore with DataFlow](#pyspark-data_catalog-hive_metastore-data_flow.ipynb)
 - [Using Data Catalog Metastore with PySpark](#pyspark-data_catalog-hive_metastore.ipynb)
 - [Using Livy on the Big Data Service](#big_data_service-(BDS)-livy.ipynb)
 - [Visual Genome Repository](#genome_visualization-oracle_open_data.ipynb)
 - [Visualizing Data](#visualizing_data-exploring_data.ipynb)
 - [Working with Pipelines](#pipelines-ml_lifecycle.ipynb)
 - [XGBoost with RAPIDS](#xgboost-with-rapids.ipynb)
 - [Medical data using feature store](#feature_store_ehr_data.ipynb)
 - [Storage of hugging face embeddings using feature store](#feature_store_embeddings.ipynb)
 - [Storage of open ai embeddings using feature store](#feature_store_embeddings_openai.ipynb)
 - [Synthetic data generation using feature store](#feature_store_medical_synthetic_data_openai.ipynb)
 - [PII redaction using feature store](#feature_store_pii_redaction_and_transformation.ipynb)
 - [Querying operations using feature store](#feature_store_querying.ipynb)
 - [Quickstart for feature store](#feature_store_quickstart.ipynb)
 - [Schema evolution and schema enforcement using feature store](#feature_store_schema_evolution.ipynb)
 - [Big data operations using feature store](#feature_store_spark_magic.ipynb)
 - [Streaming operations using feature store](#feature_store_streaming_data_frame.ipynb)


## Notebooks
### <a name="automlx-anomaly_detection.ipynb"></a> - Building and Explaining an Anomaly Detector using AutoMLx - Experimental

<sub>Updated: 05/29/2023</sub>
#### [`automlx-anomaly_detection.ipynb`](automlx-anomaly_detection.ipynb)

 
Build an anomaly detection model using the experimental, fully unsupervised anomaly detection pipeline in Oracle AutoMLx for the public Credit Card Fraud dataset.

This notebook was developed on the conda pack with slug: `automlx_p38_cpu_v2`

 
`automlx`  `anomaly detection`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="automlx-classifier.ipynb"></a> - Building and Explaining a Classifier using AutoMLx

<sub>Updated: 05/29/2023</sub>
#### [`automlx-classifier.ipynb`](automlx-classifier.ipynb)

 
Build a classifier using the Oracle AutoMLx tool and binary data set of Census income data.

This notebook was developed on the conda pack with slug: `automlx_p38_cpu_v3`

 
`automlx`  `classification`  `classifier`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="automlx-fairness.ipynb"></a> - Fairness with AutoMLx

<sub>Updated: 05/29/2023</sub>
#### [`automlx-fairness.ipynb`](automlx-fairness.ipynb)

 
Develop a model and evaluate its fairness

This notebook was developed on the conda pack with slug: `automlx_p38_cpu_v3`

 
`automlx`  `fairness`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="automlx-regression.ipynb"></a> - Building and Explaining a Regressor using AutoMLx

<sub>Updated: 05/29/2023</sub>
#### [`automlx-regression.ipynb`](automlx-regression.ipynb)

 
Build a regressor using Oracle AutoMLx and a pricing data set. Training options will be explored and the resulting AutoMLx models will be evaluated.

This notebook was developed on the conda pack with slug: `automlx_p38_cpu_v3`

 
`automlx`  `regression`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="automlx-text_classification.ipynb"></a> - Building and Explaining a Text Classifier using AutoMLx

<sub>Updated: 05/29/2023</sub>
#### [`automlx-text_classification.ipynb`](automlx-text_classification.ipynb)

 
build a classifier using the Oracle AutoMLx tool for the public 20newsgroup dataset

This notebook was developed on the conda pack with slug: `automlx_p38_cpu_v3`

 
`automlx`  `text classification`  `text classifier`

<sub>Universal Permissive License v 1.0.</sup>

---
### <a name="audi-autonomous_driving-oracle_open_data.ipynb"></a> - Audi Autonomous Driving Dataset Repository

<sub>Updated: 03/30/2023</sub>
#### [`audi-autonomous_driving-oracle_open_data.ipynb`](audi-autonomous_driving-oracle_open_data.ipynb)

 
Download, process and display autonomous driving data, and map LiDAR data onto images.

This notebook was developed on the conda pack with slug: `computervision_p37_cpu_v1`

 
`autonomous driving`  `oracle open data`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="big_data_service-(BDS)-livy.ipynb"></a> - Using Livy on the Big Data Service

<sub>Updated: 03/26/2023</sub>
#### [`big_data_service-(BDS)-livy.ipynb`](big_data_service-(BDS)-livy.ipynb)

 
Work interactively with a BDS cluster using Livy and two different connection techniques, SparkMagic (for a notebook environment) and with REST.

This notebook was developed on the conda pack with slug: `pyspark30_p37_cpu_v5`

 
`bds`  `big data service`  `livy`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="read-write-big_data_service-(BDS).ipynb"></a> - How to Read Data with fsspec from Oracle Big Data Service (BDS)

<sub>Updated: 03/29/2023</sub>
#### [`read-write-big_data_service-(BDS).ipynb`](read-write-big_data_service-(BDS).ipynb)

 
Manage data using fsspec file system. Read and save data using pandas and pyarrow through fsspec file system.

This notebook was developed on the conda pack with slug: `pyspark30_p37_cpu_v5`

 
`bds`  `fsspec`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="caltech-pedestrian_detection-oracle_open_data.ipynb"></a> - Caltech Pedestrian Detection Benchmark Repository

<sub>Updated: 03/30/2023</sub>
#### [`caltech-pedestrian_detection-oracle_open_data.ipynb`](caltech-pedestrian_detection-oracle_open_data.ipynb)

 
Download and process annotated video data of vehicles and pedestrians.

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`caltech`  `pedestrian detection`  `oracle open data`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="pyspark-data_catalog-hive_metastore-data_flow.ipynb"></a> - Using Data Catalog Metastore with DataFlow

<sub>Updated: 03/26/2023</sub>
#### [`pyspark-data_catalog-hive_metastore-data_flow.ipynb`](pyspark-data_catalog-hive_metastore-data_flow.ipynb)

 
Write and test a Data Flow batch application using the Oracle Cloud Infrastructure (OCI) Data Catalog Metastore. Configure the job, run the application and clean up resources.

This notebook was developed on the conda pack with slug: `pyspark30_p37_cpu_v5`

 
`data catalog metastore`  `data flow`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="data_labeling-text_classification.ipynb"></a> - Text Classification with Data Labeling Service Integration

<sub>Updated: 03/30/2023</sub>
#### [`data_labeling-text_classification.ipynb`](data_labeling-text_classification.ipynb)

 
Use the Oracle Cloud Infrastructure (OCI) Data Labeling service to efficiently build enriched, labeled datasets for the purpose of accurately training AI/ML models. This notebook demonstrates operations that can be performed using the Advanced Data Science (ADS) Data Labeling module.

This notebook was developed on the conda pack with slug: `nlp_p37_cpu_v2`

 
`data labeling`  `text classification`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="visualizing_data-exploring_data.ipynb"></a> - Visualizing Data

<sub>Updated: 03/30/2023</sub>
#### [`visualizing_data-exploring_data.ipynb`](visualizing_data-exploring_data.ipynb)

 
Perform common data visualization tasks and explore data with the ADS SDK. Plotting approaches include 3D plots, pie chart, GIS plots, and Seaborn pairplot graphs.

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`data visualization`  `seaborn plot`  `charts`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="pyspark-data_catalog-hive_metastore.ipynb"></a> - Using Data Catalog Metastore with PySpark

<sub>Updated: 03/30/2023</sub>
#### [`pyspark-data_catalog-hive_metastore.ipynb`](pyspark-data_catalog-hive_metastore.ipynb)

 
Configure and use PySpark to process data in the Oracle Cloud Infrastructure (OCI) Data Catalog metastore, including common operations like creating and loading data from the metastore.

This notebook was developed on the conda pack with slug: `pyspark30_p37_cpu_v5`

 
`dcat`  `data catalog metastore`  `pyspark`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="train-register-deploy-other-frameworks.ipynb"></a> - Train, Register, and Deploy a Generic Model

<sub>Updated: 03/26/2023</sub>
#### [`train-register-deploy-other-frameworks.ipynb`](train-register-deploy-other-frameworks.ipynb)

 
Train, register, and deploy a generic model

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`generic model`  `deploy model`  `register model`  `train model`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="graph_insight-autonomous_database.ipynb"></a> - Bank Graph Example Notebook

<sub>Updated: 06/05/2023</sub>
#### [`graph_insight-autonomous_database.ipynb`](graph_insight-autonomous_database.ipynb)

 
Access

This notebook was developed on the conda pack with slug: `pypgx2310_p38_cpu_v1`

 
`graph_insight`  `autonomous_database`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="train-register-deploy-huggingface-pipeline.ipynb"></a> - Train, register, and deploy HuggingFace Pipeline

<sub>Updated: 03/26/2023</sub>
#### [`train-register-deploy-huggingface-pipeline.ipynb`](train-register-deploy-huggingface-pipeline.ipynb)

 
Train, register, and deploy a huggingface pipeline.

This notebook was developed on the conda pack with slug: `pytorch110_p38_cpu_v1`

 
`huggingface`  `deploy model`  `register model`  `train model`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="hyperparameter_tuning.ipynb"></a> - Introduction to ADSTuner

<sub>Updated: 03/30/2023</sub>
#### [`hyperparameter_tuning.ipynb`](hyperparameter_tuning.ipynb)

 
Use ADSTuner to optimize an estimator using the scikit-learn API

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`hyperparameter tuning`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="accelerate-scikit_learn-with-intel_extension.ipynb"></a> - Intel Extension for Scikit-Learn

<sub>Updated: 03/26/2023</sub>
#### [`accelerate-scikit_learn-with-intel_extension.ipynb`](accelerate-scikit_learn-with-intel_extension.ipynb)

 
Enhance performance of scikit-learn models using the Intel(R) oneAPI Data Analytics Library. Train a k-means model using both sklearn and the accelerated Intel library and compare performance.

This notebook was developed on the conda pack with slug: `sklearnex202130_p37_cpu_v1`

 
`intel`  `intel extension`  `scikit-learn`  `scikit learn`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="big_data_service-(BDS)-kerberos.ipynb"></a> - Connect to Oracle Big Data Service

<sub>Updated: 03/27/2023</sub>
#### [`big_data_service-(BDS)-kerberos.ipynb`](big_data_service-(BDS)-kerberos.ipynb)

 
Connect to Oracle Big Data services using Kerberos.

This notebook was developed on the conda pack with slug: `pyspark30_p37_cpu_v5`

 
`kerberos`  `big data service`  `bds`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="natural_language_processing.ipynb"></a> - Natural Language Processing

<sub>Updated: 03/26/2023</sub>
#### [`natural_language_processing.ipynb`](natural_language_processing.ipynb)

 
Use the ADS SDK to process and manipulate strings. This notebook includes regular expression matching and natural language (NLP) parsing, including part-of-speech tagging, named entity recognition, and sentiment analysis. It also shows how to create and use custom plugins specific to your specific needs.

This notebook was developed on the conda pack with slug: `nlp_p37_cpu_v2`

 
`language services`  `string manipulation`  `regex`  `regular expression`  `natural language processing`  `NLP`  `part-of-speech tagging`  `named entity recognition`  `sentiment analysis`  `custom plugins`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="automlx-forecasting.ipynb"></a> - Building a Forecaster using AutoMLx

<sub>Updated: 05/29/2023</sub>
#### [`automlx-forecasting.ipynb`](automlx-forecasting.ipynb)

 
Use Oracle AutoMLx to build a forecast model with real-world data sets.

This notebook was developed on the conda pack with slug: `automlx_p38_cpu_v3`

 
`language services`  `string manipulation`  `regex`  `regular expression`  `natural language processing`  `NLP`  `part-of-speech tagging`  `named entity recognition`  `sentiment analysis`  `custom plugins`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="train-register-deploy-lightgbm.ipynb"></a> - Train, Register, and Deploy a LightGBM Model

<sub>Updated: 03/26/2023</sub>
#### [`train-register-deploy-lightgbm.ipynb`](train-register-deploy-lightgbm.ipynb)

 
Train, register, and deploy a LightGBM model.

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`lightgbm`  `deploy model`  `register model`  `train model`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="load_data-object_storage-hive-autonomous-database.ipynb"></a> - Loading Data With Pandas & Dask

<sub>Updated: 03/26/2023</sub>
#### [`load_data-object_storage-hive-autonomous-database.ipynb`](load_data-object_storage-hive-autonomous-database.ipynb)

 
Load data from sources including ADW, Object Storage, and Hive in formats like parquet, csv etc

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`loading data`  `autonomous database`  `adw`  `hive`  `pandas`  `dask`  `object storage`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="model_version_set.ipynb"></a> - Introduction to Model Version Set

<sub>Updated: 03/26/2023</sub>
#### [`model_version_set.ipynb`](model_version_set.ipynb)

 
A model version set is a way to track the relationships between models. As a container, the model version set takes a collection of models. Those models are assigned a sequential version number based on the order they are entered into the model version set.

This notebook was developed on the conda pack with slug: `dbexp_p38_cpu_v1`

 
`model`  `model experiments`  `model version set`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="model_evaluation-with-ADSEvaluator.ipynb"></a> - Model Evaluation with ADSEvaluator

<sub>Updated: 03/30/2023</sub>
#### [`model_evaluation-with-ADSEvaluator.ipynb`](model_evaluation-with-ADSEvaluator.ipynb)

 
Train and evaluate different types of models: binary classification using an imbalanced dataset, multi-class classification using a synthetically generated dataset consisting of three equally distributed classes, and a regression using a synthetically generated dataset with positive targets.

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`model evaluation`  `binary classification`  `regression`  `multi-class classification`  `imbalanced dataset`  `synthetic dataset`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="text_classification-model_explanation-lime.ipynb"></a> - Text Classification and Model Explanations using LIME

<sub>Updated: 03/30/2023</sub>
#### [`text_classification-model_explanation-lime.ipynb`](text_classification-model_explanation-lime.ipynb)

 
Perform model explanations on an NLP classifier using the locally interpretable model explanations technique (LIME).

This notebook was developed on the conda pack with slug: `nlp_p37_cpu_v2`

 
`nlp`  `lime`  `model_explanation`  `text_classification`  `text_explanation`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="genome_visualization-oracle_open_data.ipynb"></a> - Visual Genome Repository

<sub>Updated: 03/30/2023</sub>
#### [`genome_visualization-oracle_open_data.ipynb`](genome_visualization-oracle_open_data.ipynb)

 
Load visual data, define regions, and visualize objects using metadata to connect structured images to language.

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`object annotation`  `genome visualization`  `oracle open data`

<sub>Universal Permissive License v 1.0 (https://oss.oracle.com/licenses/upl/)</sup>

---
### <a name="onnx-integration-ads.ipynb"></a> - ONNX Integration with the Accelerated Data Science (ADS) SDK

<sub>Updated: 07/17/2023</sub>
#### [`onnx-integration-ads.ipynb`](onnx-integration-ads.ipynb)

 
Extract text from common formats (e.g. PDF and Word) into plain text. Customize this process for individual use cases.

This notebook was developed on the conda pack with slug: `nlp_p37_cpu_v2`

 
`onnx`  `deploy model`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="pipelines-ml_lifecycle.ipynb"></a> - Working with Pipelines

<sub>Updated: 03/26/2023</sub>
#### [`pipelines-ml_lifecycle.ipynb`](pipelines-ml_lifecycle.ipynb)

 
Create and use ML pipelines through the entire machine learning lifecycle

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`pipelines`  `pipeline step`  `jobs pipeline`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="pypgx-graph_analytics-machine_learning.ipynb"></a> - Graph Analytics and Graph Machine Learning with PyPGX

<sub>Updated: 03/26/2023</sub>
#### [`pypgx-graph_analytics-machine_learning.ipynb`](pypgx-graph_analytics-machine_learning.ipynb)

 
Use Oracle's Graph Analytics libraries to demonstrate graph algorithms, graph machine learning models, and use the property graph query language (PGQL)

This notebook was developed on the conda pack with slug: `pypgx2310_p38_cpu_v1`

 
`pypgx`  `graph analytics`  `pgx`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="pyspark-data_flow_studio-introduction.ipynb"></a> - Introduction to the Oracle Cloud Infrastructure Data Flow Studio

<sub>Updated: 03/26/2023</sub>
#### [`pyspark-data_flow_studio-introduction.ipynb`](pyspark-data_flow_studio-introduction.ipynb)

 
Run interactive Spark workloads on a long lasting Oracle Cloud Infrastructure Data Flow Spark cluster through Apache Livy integration. Data Flow Spark Magic is used for interactively working with remote Spark clusters through Livy, a Spark REST server, in Jupyter notebooks. It includes a set of magic commands for interactively running Spark code.

This notebook was developed on the conda pack with slug: `pyspark32_p38_cpu_v2`

 
`pyspark`  `data flow`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="pyspark-data_flow_studio-spark_nlp.ipynb"></a> - Spark NLP within Oracle Cloud Infrastructure Data Flow Studio

<sub>Updated: 03/26/2023</sub>
#### [`pyspark-data_flow_studio-spark_nlp.ipynb`](pyspark-data_flow_studio-spark_nlp.ipynb)

 
Demonstrates how to use Spark NLP within a long lasting Oracle Cloud Infrastructure Data Flow cluster.

This notebook was developed on the conda pack with slug: `pyspark32_p38_cpu_v1`

 
`pyspark`  `data flow`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="pyspark-data_flow-application.ipynb"></a> - PySpark

<sub>Updated: 06/02/2023</sub>
#### [`pyspark-data_flow-application.ipynb`](pyspark-data_flow-application.ipynb)

 
Develop local PySpark applications and work with remote clusters using Data Flow.

This notebook was developed on the conda pack with slug: `pyspark24_p37_cpu_v3`

 
`pyspark`  `data flow`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="train-register-deploy-pytorch.ipynb"></a> - Train, Register, and Deploy a PyTorch Model

<sub>Updated: 03/26/2023</sub>
#### [`train-register-deploy-pytorch.ipynb`](train-register-deploy-pytorch.ipynb)

 
Train, register, and deploy a PyTorch model.

This notebook was developed on the conda pack with slug: `pytorch110_p38_cpu_v1`

 
`pytorch`  `deploy model`  `register model`  `train model`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="train-register-deploy-sklearn.ipynb"></a> - Train, register, and deploy Sklearn Model

<sub>Updated: 03/26/2023</sub>
#### [`train-register-deploy-sklearn.ipynb`](train-register-deploy-sklearn.ipynb)

 
Train, register, and deploy an scikit-learn model.

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`scikit-learn`  `deploy model`  `register model`  `train model`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="sql_magic-commands-with-autonomous_database.ipynb"></a> - Introduction to SQL Magic

<sub>Updated: 03/30/2023</sub>
#### [`sql_magic-commands-with-autonomous_database.ipynb`](sql_magic-commands-with-autonomous_database.ipynb)

 
Use SQL Magic commands to work with a database within a Jupyter notebook. This notebook shows how to to use both line and cell magics.

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`sql magic`  `autonomous database`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="streaming-service-introduction.ipynb"></a> - Introduction to Streaming

<sub>Updated: 03/30/2023</sub>
#### [`streaming-service-introduction.ipynb`](streaming-service-introduction.ipynb)

 
Connect to Oracle Cloud Insfrastructure (OCI) Streaming service with kafka.

This notebook was developed on the conda pack with slug: `dataexpl_p37_cpu_v3`

 
`streaming`  `kafka`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="train-register-deploy-tensorflow.ipynb"></a> - Train, Register, and Deploy a TensorFlow Model

<sub>Updated: 03/26/2023</sub>
#### [`train-register-deploy-tensorflow.ipynb`](train-register-deploy-tensorflow.ipynb)

 
Train, register, and deploy a TensorFlow model.

This notebook was developed on the conda pack with slug: `tensorflow28_p38_cpu_v1`

 
`tensorflow`  `deploy model`  `register model`  `train model`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="document-text_extraction.ipynb"></a> - Text Extraction Using the Accelerated Data Science (ADS) SDK

<sub>Updated: 03/26/2023</sub>
#### [`document-text_extraction.ipynb`](document-text_extraction.ipynb)

 
Extract text from common formats (e.g. PDF and Word) into plain text. Customize this process for individual use cases.

This notebook was developed on the conda pack with slug: `nlp_p37_cpu_v2`

 
`text extraction`  `nlp`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="train-register-deploy-xgboost.ipynb"></a> - Train, Register, and Deploy an XGBoost Model

<sub>Updated: 03/26/2023</sub>
#### [`train-register-deploy-xgboost.ipynb`](train-register-deploy-xgboost.ipynb)

 
Train, register, and deploy an XGBoost model.

This notebook was developed on the conda pack with slug: `generalml_p38_cpu_v1`

 
`xgboost`  `deploy model`  `register model`  `train model`

<sub>Universal Permissive License v 1.0</sup>

---
### <a name="xgboost-with-rapids.ipynb"></a> - XGBoost with RAPIDS

<sub>Updated: 03/30/2023</sub>
#### [`xgboost-with-rapids.ipynb`](xgboost-with-rapids.ipynb)

 
Compare training time between CPU and GPU trained models using XGBoost

This notebook was developed on the conda pack with slug: `rapids2110_p37_gpu_v1`

 
`xgboost`  `rapids`  `gpu`  `machine learning`  `classification`

<sub>Universal Permissive License v 1.0</sup>

---

### <a name="feature_store_ehr_data.ipynb"></a> - Medical Data Management Using Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_ehr_data.ipynb`](feature_store_ehr_data.ipynb)

Manage and utilize medical data efficiently using a feature store. This notebook demonstrates the storage, retrieval, and manipulation of Electronic Health Record (EHR) data within a feature store framework.

`feature store` `medical data` `data management` 

<sub>License: Universal Permissive License v 1.0</sub>

---

### <a name="feature_store_embeddings.ipynb"></a> - Storage of Hugging Face Embeddings Using Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_embeddings.ipynb`](feature_store_embeddings.ipynb)

Explore the storage and retrieval of Hugging Face embeddings within a feature store setup. This notebook provides insights into storing and utilizing pre-trained embeddings for various natural language processing tasks using the feature store infrastructure.

`feature store` `Hugging Face embeddings` `storage`

<sub>License: Universal Permissive License v 1.0</sub>

---

### <a name="feature_store_embeddings_openai.ipynb"></a> - Storage of OpenAI Embeddings Using Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_embeddings_openai.ipynb`](feature_store_embeddings_openai.ipynb)

Learn how to store and leverage OpenAI embeddings effectively within a feature store environment. This notebook guides users through the process of managing and utilizing OpenAI-generated embeddings for diverse machine learning applications within a feature store framework.

`feature store` `OpenAI embeddings` `storage`

<sub>License: Universal Permissive License v 1.0</sub>

---

### <a name="feature_store_medical_synthetic_data_openai.ipynb"></a> - Synthetic Data Generation Using Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_medical_synthetic_data_openai.ipynb`](feature_store_medical_synthetic_data_openai.ipynb)

Generate synthetic medical data leveraging OpenAI tools within a feature store. This notebook illustrates the process of creating synthetic medical data for various research and analysis purposes using the capabilities of a feature store.

`feature store` `synthetic data generation` `medical data` 

<sub>License: Universal Permissive License v 1.0</sub>

---

### <a name="feature_store_pii_redaction_and_transformation.ipynb"></a> - PII Redaction Using Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_pii_redaction_and_transformation.ipynb`](feature_store_pii_redaction_and_transformation.ipynb)

Explore techniques and methods for Personally Identifiable Information (PII) redaction and transformation within a feature store environment. This notebook demonstrates how to manage sensitive data securely by implementing PII masking and transformation techniques using a feature store.

`feature store` `PII redaction` `data security`

<sub>License: Universal Permissive License v 1.0</sub>

---

### <a name="feature_store_querying.ipynb"></a> - Querying Operations Using Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_querying.ipynb`](feature_store_querying.ipynb)

Understand and perform querying operations within a feature store setup. This notebook covers querying techniques, data retrieval, and manipulation strategies to efficiently access and utilize stored features in a feature store environment.

`feature store` `querying` `data operations`

<sub>License: Universal Permissive License v 1.0</sub>

---

### <a name="feature_store_quickstart.ipynb"></a> - Quickstart for Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_quickstart.ipynb`](feature_store_quickstart.ipynb)

Get started quickly with a feature store setup using this introductory notebook. It provides step-by-step guidance and essential information for setting up and utilizing a feature store environment for efficient data management and analysis.

`feature store` `quickstart` `data management`

<sub>License: Universal Permissive License v 1.0</sub>

---

### <a name="feature_store_schema_evolution.ipynb"></a> - Schema Evolution and Schema Enforcement Using Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_schema_evolution.ipynb`](feature_store_schema_evolution.ipynb)

Learn about schema evolution and enforcement techniques within a feature store. This notebook explores methods to handle schema changes, enforce data integrity, and manage evolving data structures effectively in a feature store environment.

`feature store` `schema evolution` `data integrity`

<sub>License: Universal Permissive License v 1.0</sub>

---

### <a name="feature_store_spark_magic.ipynb"></a> - Big Data Operations Using Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_spark_magic.ipynb`](feature_store_spark_magic.ipynb)

Explore big data operations within a feature store using Spark magic commands. This notebook demonstrates how to leverage the power of Spark for efficient data handling and analysis in a feature store environment.

`feature store` `big data operations` `Spark`

<sub>License: Universal Permissive License v 1.0</sub>

---

### <a name="feature_store_streaming_data_frame.ipynb"></a> - Streaming Operations Using Feature Store

<sub>Updated: 11/13/2023</sub>
#### [`feature_store_streaming_data_frame.ipynb`](feature_store_streaming_data_frame.ipynb)

Explore streaming operations within a feature store using Spark. This notebook demonstrates how to leverage the power of Spark Streaming for efficient data handling and analysis in a feature store environment.

`feature store` `big data operations` `Spark` `Spark Streaming`

<sub>License: Universal Permissive License v 1.0</sub>

---

